---
title: '유사도의 척도, 거리의 종류'
author: 'Dr.Kevin'
date: '5/31/2018'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300)
```

일부 기계학습 알고리즘은 **데이터 간 유사도 혹은 비유사도를 측정**하는 경우가 있습니다. 유사도의 기준으로는 주로 **거리(Distance)를** 이용하는데요. 예를 들어 군집분석의 경우, 데이터 간 거리가 가까울수록 유사도가 높다고 판단하여 같은 군집으로 묶습니다. 지도학습의 K-근접이웃 역시 데이터 간 거리를 측정하고 가장 가까운 이웃들의 목표변수를 기준으로 새로운 데이터의 목표변수를 추론합니다. 

## 거리(Distance)의 특징 

함수 $ d(x, y) $를 두 점 x와 y의 거리는 반환하는 함수라고 가정했을 때, $ d(x, y) $는 다음과 같은 특징을 갖습니다.  

  - 모든 거리는 0보다 크거나 같습니다.  
  
    $$ d(x, y) \; \geq \; 0 $$
    
  - 교환법칙이 성립합니다.  
  
    $$ d(x, y) \; = \; d(y, x) $$
  
  - 다른 한 점 z를 경유하는 거리의 합보다 작거나 같습니다. (삼각 부등식)  
  
    $$ d(x, y) \; \leq \; d(x, z) \; + \; d(y, z) $$

## 데이터 표준화와 정규화 

기계학습 알고리즘에서 거리 계산이 포함되는 경우, 반드시 데이터 전처리 과정에서 **표준화 (Standardizatoin)**를 해주어야 합니다. 표준화는 데이터 `x`를 평균이 0이고, 표준편차가 1인 표준정규분포를 따르는 `z-score`로 변환하는 것을 의미합니다. 

  $$ \text {z-score} = \frac {x - \mu}{\sigma} $$

R에서는 `scale()` 함수를 사용하여 손쉽게 표준화할 수 있습니다. `scale()` 함수의 주요 인자로는 `x`, `center`, `scale` 등 3가지가 있습니다.  

  - `x` : 변환하려는 숫자형 벡터를 할당합니다.  
  - `center` : 위 식에서 분자의 $ \mu $에 해당하는 `x`의 평균을 할당합니다. 기본값으로 `TRUE`가 설정되어 있는데, 표준화 데이터의 평균을 0으로 할 때는 생략해도 무방합니다.  
  - `scale` : 위 식에서 분자의 $ \sigma $에 해당하는 `x`의 표준편차를 할당합니다. 기본값으로 `TRUE`가 설정되어 있는데, 표준화 데이터의 표준편차를 1로 할 때는 생략해도 무방합니다.  

그런데 표준화 대신 정규화를 하는 경우가 있으니 참고로 소개해드리겠습니다. 정규화는 데이터 `x`를 0~1 사이의 값을 갖도록 변환하는 것입니다. 위 식의 분자 항목의 $ \mu $ 대신 `최소값`을 넣고, 분모 항목의 $ \sigma $ 대신 `(최대값 - 최소값)`을 넣으면 됩니다. 

  $$ \text {x-normalized} = \frac {x - \text{min}}{\text {max - min}} $$

정규화 역시 `scale()` 함수를 이용하면 되며, 표준화와 달리 `center`, `scale` 인자에 반드시 해당하는 데이터를 할당해주어야 합니다.  

  - `center` : `min(x)`를 할당합니다.  
  - `scale` : (`max(x)` - `min(x)`)를 할당합니다.  

## 거리의 종류 

p차원의 공간에 두 점 $ A(x_1, x_2, \cdots , x_p) $와 $ B(y_1, y_2, \cdots , y_p) $가 있다고 할 때, 두 점 사이의 거리는 종류에 따라 다음과 같이 계산할 수 있습니다.  

  - 맨하탄 거리 : 두 점 간 차의 절대값을 합한 값으로 $ L_1 $ norm이라고 합니다. 격자 모양의 거리를 자동차로 운행하는 거리를 연상하면 됩니다. **택시거리**라는 별칭을 가지고 있습니다.  
  
    $$ d(A, B) = \| x_i - y_i \|_1 = \sum_{i=1}^{p} \; | x_i - y_i | $$  
  
  - 유클리드 거리 : 두 점 간 차를 제곱하여 모두 더한 값의 양의 제곱근으로 $ L_2 $ norm이라고 합니다.
  
    $$ d(A, B) = \| x_i - y_i \|_2 = \sqrt [2] {\sum_{i=1}^{p} \; (x_i - y_i)^2} = \biggl[ \sum_{i=1}^{p} \; (x_i - y_i)^2 \biggl]^{\frac {1}{2}} $$  
  
  - 민코프스키 거리 : m차원 민코프스키 공간에서의 거리입니다. m=1일 때 맨하탄 거리와 같고, m=2일 때 유클리드 거리와 같습니다. m이 정수가 아니어도 되지만 반드시 1보다 커야 합니다.  
  
    $$ d(A, B) = \sqrt [m] {\sum_{i=1}^{p} \; (x_i - y_i)^m} = \biggl[ \sum_{i=1}^{p} \; (x_i - y_i)^m \biggl]^{\frac {1}{m}} $$  
  
  - 맥시멈 거리 : 민코프스키 거리에서 m을 무한대로 확장한 거리입니다. 체비셰프(Chebyshev) 거리라고도 하며, 두 집단에서 가장 긴 지점에서의 거리를 의미합니다.  
  
    $$ d(A, B) = \lim_{m \to \infty} \biggl[ \sum_{i=1}^{p} \; (x_i - y_i)^m \biggl]^{\frac {1}{m}} = \max \biggl( {| x_1 - y_1 |}, \; {| x_2 - y_2 |}, \; \cdots\; , \; {| x_p - y_p |} \biggl) $$
  
  - 표준화 거리 : 유클리드 거리를 공분산으로 나눈 거리입니다. 공분산을 원소로 갖는 대각행렬($D$)을 이용합니다.  
  
    $$ d(A, B) = \biggl[ \sum_{i=1}^{p} \; \frac {(x_i - y_i)^2}{\sigma_{ii}} \biggl]^{\frac {1}{2}} = \biggl[ (X - Y)^T D^{-1} (X - Y) \biggl]^{\frac{1}{2}} $$  
    $$ D = \left[ \begin{array}{ccc} \sigma_{11} & 0 & \cdots & 0 \\ 0 & \sigma_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sigma_{pp} \\ \end{array} \right] $$
  
  - 캔버라 거리 : 가중치 있는 맨하탄 거리입니다. 원점 주변에 흩어져 있는 데이터에 주로 사용됩니다.  

    $$ \text {d(A, B)} = \sum_{i=1}^{p} \frac { | {x_i - y_i} | } { | x_i | } + { | y_i | }} $$

  - 마할라노비스 거리 : 확률분포를 고려해야 할 때, 공분산 행렬($ \Sigma^{-1} $)을 이용합니다.

    $$ \text {d(A, B)} = \biggl[ (X - Y)^T \Sigma^{-1} (X - Y) \biggl]^{\frac {1}{2}} $$
    $$ S = \left[ \begin{array}{ccc} \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p} \\ \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{pp} \\ \end{array} \right] $$

  - 코사인 유사도 : 두 벡터이 내적을 각 벡터의 크기로 나눈 값을 1에서 뺀 것입니다. 코사인은 0 ~ 1 사이의 값을 갖는데, 0도 일 때 1, 90도일 때 0의 값을 갖는다는 점을 착안한다면 두 벡터의 사이각이 0에 가까울수록 두 벡터의 거리가 가깝다고 판단할 수 있습니다.  
  
    $$ d(A, B) = 1 - \frac { \langle x, \; y \rangle} {\| x \|_2 \|y\|_2} = 1 - \frac {\sum_{i=1}^{p} x_i y_i}{\sqrt [2] {\sum_{i=1}^{p} x_i^2} \sqrt [2] {\sum_{i=1}^{p} y_i^2} } $$

  - 피어슨 거리 : 피어슨 상관계수가 -1 ~ 1 사이의 값을 가지므로, 피어슨 상관계수를 1에서 뺀 값은 0 ~ 2 사이의 값을 갖게 됩니다.  
  
    $$ d(A, B) = 1 - \text {Corr}(x, y) $$  

이상으로 기계학습에서 유사도의 기준으로 삼는 거리의 특징과 종류를 알아봤습니다. 
