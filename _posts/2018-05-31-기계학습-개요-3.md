유사도의 척도, 거리의 종류
================
Dr.Kevin
5/31/2018

일부 기계학습 알고리즘은 **데이터 간 유사도 혹은 비유사도를 측정**하는 경우가 있습니다. 유사도의 기준으로는 주로 **거리(Distance)**를 이용하는데요. 예를 들어 군집분석의 경우, 데이터 간 거리가 가까울수록 유사도가 높다고 판단하여 같은 군집으로 묶습니다. 지도학습의 K-근접이웃 역시 데이터 간 거리를 측정하고 가장 가까운 이웃들의 목표변수를 기준으로 새로운 데이터의 목표변수를 추론합니다.

거리(Distance)의 특징
---------------------

함수 $$ d(x, y) $$를 두 점 x와 y의 거리는 반환하는 함수라고 가정했을 때, $$ d(x, y) $$는 다음과 같은 특징을 갖습니다.

-   모든 거리는 0보다 크거나 같습니다.

  $$ d(x, y) ≥ 0 $$

-   교환법칙이 성립합니다.

  $$ d(x, y) = d(y, x) $$

-   다른 한 점 z를 경유하는 거리의 합보다 작거나 같습니다. (삼각 부등식)

  $$ d(x, y) ≤ d(x, z) + d(y, z) $$

데이터 표준화와 정규화
----------------------

기계학습 알고리즘에서 거리 계산이 포함되는 경우, 반드시 데이터 전처리 과정에서 **표준화 (Standardizatoin)**를 해주어야 합니다. 표준화는 데이터 `x`를 평균이 0이고, 표준편차가 1인 표준정규분포를 따르는 `z-score`로 변환하는 것을 의미합니다.

  $$ \text {z-score} = \frac {x - \mu}{\sigma} $$

R에서는 `scale()` 함수를 사용하여 손쉽게 표준화할 수 있습니다. `scale()` 함수의 주요 인자로는 `x`, `center`, `scale` 등 3가지가 있습니다.

-   `x` : 변환하려는 숫자형 벡터를 할당합니다.
-   `center` : 위 식에서 분자의 $$ \mu $$에 해당하는 `x`의 평균을 할당합니다. 기본값으로 `TRUE`가 설정되어 있는데, 표준화 데이터의 평균을 0으로 할 때는 생략해도 무방합니다.
-   `scale` : 위 식에서 분자의 $$ \sigma $$에 해당하는 `x`의 표준편차를 할당합니다. 기본값으로 `TRUE`가 설정되어 있는데, 표준화 데이터의 표준편차를 1로 할 때는 생략해도 무방합니다.

그런데 표준화 대신 정규화를 하는 경우가 있으니 참고로 소개해드리겠습니다. 정규화는 데이터 `x`를 0~1 사이의 값을 갖도록 변환하는 것입니다. 위 식에서 분자의 $$ \mu $$ 대신 `최소값`을 넣고, 분모의 $$ \sigma $$ 대신 `(최대값 - 최소값)`을 넣으면 됩니다.

  $$ \text {x-normalized} = \frac {x - \text{min}}{\text {max - min}} $$

정규화 역시 `scale()` 함수를 이용하면 되며, 표준화와 달리 `center`, `scale` 인자에 반드시 해당하는 데이터를 할당해주어야 합니다.

-   `center` : `min(x)`를 할당합니다.
-   `scale` : (`max(x)` - `min(x)`)를 할당합니다.

거리의 종류
-----------

p차원의 공간에 두 점 $$ A(x_1, x_2, \cdots, x_p) $$와 $$ B(y_1, y_2, \cdots, y_p) $$가 있다고 할 때, 두 점 사이의 거리는 종류에 따라 다음과 같이 계산할 수 있습니다.

-   맨하탄 거리 : 두 점 간 차의 절대값을 합한 값으로 $$ \text {L}_1 $$ norm이라고 합니다. 격자 모양의 거리를 자동차로 운행하는 거리를 연상하면 됩니다. **택시거리**라는 별칭을 가지고 있습니다.

  $$ d(A, B) = \| x_i - y_i \|_1 = \sum_{i=1}^{p} | x_i - y_i | $$

-   유클리드 거리 : 두 점 간 차를 제곱하여 모두 더한 값의 양의 제곱근으로 $$ \text {L}_2 $$ norm이라고 합니다.

  $$ d(A, B) = \| x_i - y_i \|_2 = \sqrt [2] {\sum_{i=1}^{p} (x_i - y_i)^2} = \biggl[ \sum_{i=1}^{p} (x_i - y_i)^2 \biggl]^{\frac {1}{2}} $$

-   민코프스키 거리 : m차원 민코프스키 공간에서의 거리입니다. m=1일 때 맨하탄 거리와 같고, m=2일 때 유클리드 거리와 같습니다. m이 정수가 아니어도 되지만 반드시 1보다 커야 합니다.

  $$ d(A, B) = \sqrt [m] {\sum_{i=1}^{p} (x_i - y_i)^m} = \biggl[ \sum_{i=1}^{p} (x_i - y_i)^m \biggl]^{\frac {1}{m}} $$

-   맥시멈 거리 : 민코프스키 거리에서 m을 무한대로 확장한 거리입니다. 체비셰프(Chebyshev) 거리라고도 하며, 두 집단에서 가장 긴 지점에서의 거리를 의미합니다.

  $$ d(A, B) = \lim_{m \to \infty} \biggl[ \sum_{i=1}^{p} (x_i - y_i)^m \biggl]^{\frac {1}{m}} = \max \biggl( {| x_1 - y_1 |}, \; {| x_2 - y_2 |}, \; \cdots \; , \; {| x_p - y_p |} \biggl) $$

-   표준화 거리 : 유클리드 거리를 공분산으로 나눈 거리입니다. 공분산을 원소로 갖는 대각행렬($$ D $$)을 이용합니다.

  $$ d(A, B) = \biggl[ \sum_{i=1}^{p} \frac {(x_i - y_i)^2}{\sigma_{ii}} \biggl]^{\frac {1}{2}} = \biggl[ (X - Y)^T D^{-1} (X - Y) \biggl]^{\frac{1}{2}} $$

  $$ D = \left[ \begin{array}{ccc} \sigma_{11} & 0 & \cdots & 0 \\ 0 & \sigma_{22} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & \sigma_{pp} \\ \end{array} \right] $$

-   캔버라 거리 : 가중치 있는 맨하탄 거리입니다. 원점 주변에 흩어져 있는 데이터에 주로 사용됩니다.

  $$ \text {d(A, B)} = \sum_{i=1}^{p} \frac { | x_i - y_i | } { | x_i |  +  | y_i | } $$

-   마할라노비스 거리 : 확률분포를 고려해야 할 때, 공분산 행렬($$ \Sigma^{-1} $$)을 이용합니다.

  $$ \text {d(A, B)} = \biggl[ (X - Y)^T \Sigma^{-1} (X - Y) \biggl]^{\frac {1}{2}} $$

  $$ \Sigma = \left[ \begin{array}{ccc} \sigma_{11} & \sigma_{12} & \cdots & \sigma_{1p} \\ \sigma_{21} & \sigma_{22} & \cdots & \sigma_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ \sigma_{p1} & \sigma_{p2} & \cdots & \sigma_{pp} \\ \end{array} \right] $$

-   코사인 유사도 : 두 벡터이 내적을 각 벡터의 크기로 나눈 값을 1에서 뺀 것입니다. 코사인은 0 ~ 1 사이의 값을 갖는데, 0도 일 때 1, 90도일 때 0의 값을 갖는다는 점을 착안한다면 두 벡터의 사이각이 0에 가까울수록 두 벡터의 거리가 가깝다고 판단할 수 있습니다.

  $$ d(A, B) = 1 - \frac { \langle x, \; y \rangle} {\| x \|_2 \|y\|_2} = 1 - \frac {\sum_{i=1}^{p} x_i y_i}{\sqrt [2] {\sum_{i=1}^{p} x_i^2} \sqrt [2] {\sum_{i=1}^{p} y_i^2} } $$

-   피어슨 거리 : 피어슨 상관계수가 -1 ~ 1 사이의 값을 가지므로, 피어슨 상관계수를 1에서 뺀 값은 0 ~ 2 사이의 값을 갖게 됩니다.

  $$ d(A, B) = 1 − \text {Corr} (x, y) $$

이상으로 기계학습에서 유사도의 기준으로 삼는 거리의 특징과 종류를 알아봤습니다.
