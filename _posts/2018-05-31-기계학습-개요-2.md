분류모형의 성능 평가 기준
================
Dr.Kevin
5/31/2018

분류모형의 성능을 평가하는 기준은 여러 가지가 있겠으나 일반적으로 많이 사용되는 두 가지 방법에 대해서 소개해드리겠습니다.

**혼동행렬 (Confusion Matrix)**
-------------------------------

이전 포스팅에서 정확성(Accuracy)을 기준으로 분류모형의 성능을 평가하면 안 되는 점에 대해서 말씀드렸습니다. 분류모형의 성능을 나타내는 여러 가지 지표를 한 번에 확인할 수 있는 것으로 **혼동행렬 (Confusion Matrix)**을 사용합니다.

혼동행렬 그 자체는 비교적 이해하기 쉬운 편이라 할 수 있지만, 각종 평가 지표들은 처음 보는 사람들에게 다소 복잡해 보입니다. 먼저 혼동행렬이 어떻게 생겼는지 확인한 후 각종 지표들에 대해서 소개해드리겠습니다.

![혼동행렬 이미지](https://i.ytimg.com/vi/AOIkPnKu0YA/maxresdefault.jpg)[1]

위의 이미지를 보면, `table()` 함수를 이용하여 목표변수의 실제값(Actual)과 추정값(Estimated) 또는 예측값(Predicted)으로 빈도수를 확인하는 것과 같다는 것을 알 수 있습니다. 다만, 4개의 칸을 **True Positive**, **False Positive**, **False Negative**, 그리고 **True Negative**로 지정하고 이 4가지 항목을 가지고 다양한 지표들을 계산한다는 특징이 있습니다. 하나씩 살펴보도록 하겠습니다.

-   TP (True Positive) : 앞의 `True`는 실제값과 예측값이 같다는 것을 의미하고 뒤의 `Positive`는 예측값의 범주를 의미합니다. 목표변수에서 어떤 범주가 `Positive`인지는 혼동행렬을 실행한 결과 메시지에서 확인할 수 있습니다.
-   FP (False Positive) : 앞의 `False`는 실제값과 에측값이 서로 다르고, 모형은 `Positive`로 예측했다는 것을 의미합니다. 즉, 모형이 부정적인 사실을 긍정적인 것으로 오분류했다는 것이죠. 이것을 **제1종 오류 (Type 1 Error)**라고도 합니다. 예를 들어 암 검진을 한다고 했을 때 실제로 암환자가 정상으로 분류되는 경우를 말합니다.
-   FN (False Negative) : 앞의 `False`는 실제값과 에측값이 서로 다르고, 모형은 `Negative`로 예측했다는 것을 의미합니다. 이것을 **제2종 오류 (Type 2 Error)**라고도 하는데, 암 검진의 예를 들면 정상인을 암환자로 분류하는 경우를 말합니다. 재검을 받으면 정상으로 분류될 가능성이 높겠죠? 제1종 오류와 제2종 오류는 Positive가 어떤 범주인지에 따라 경중이 정해진다고 할 수 있습니다.
-   TN (True Negative) : 앞의 `True`는 실제값과 예측값이 같다는 것이고, 모형은 `Negative`로 예측했다는 것을 의미합니다.

이렇게 4가지 항목에 대해 충분히 이해를 했다면 다음의 내용을 반드시 알아두어야 합니다.

-   실제값이 긍정인 건수의 합 (P) = TP + FN
-   실제값이 부정인 건수의 합 (N) = FP + TN
-   전체 건수의 합 (T) = TP + FN + FP + TN = P + N

혼동행렬을 보면 쉽게 고개를 끄덕일 수 있지만 혼동행렬 없이 기호만 봐서는 언뜻 이해가 되지 않을 수 있습니다. (실은 제가 그랬습니다. ㅎㅎ) 그러니 반복 확인하여 꼭 암기하기 바랍니다.

### 혼동행렬을 통해 계산되는 분류 성능 평가 지표들[2]

-   **정확도** : 예측값과 실제값이 서로 같은 개수를 전체 합으로 나눈 것

  $$ \text {Accuracy(ACC)} = \frac {\text {TP + TN}} {\text {P + N}} $$

-   **민감도** : 실제값이 긍정인 것 중에서 모형이 맞춘 비율

  $$ \text {Sensitivity or True Positive Rate(TPR)} = \frac {\text {TP}} {\text {P}} $$

-   **정밀도** : 모형이 긍정이라고 한 것 중에서 실제값이 긍정인 비율

  $$ \text {Precision or Positive Predictive Value(PPV)} = \frac {\text {TP}} {\text {TP + FP}} $$

-   **특이도** : 실제값이 부정인 것 중에서 모형이 맞춘 비율

  $$ \text {Specificity 또는 True Negative Rate(TNR)} = \frac {\text {TN}} {\text {N}} $$

-   **1-특이도** : 실제값이 부정인 것 중에서 모형이 긍정으로 오분류한 비율

  $$ \text {False Positive Rate(FPR)} = \frac {\text {FP}} {\text {N}} $$

-   **F1 점수** : 민감도와 정밀도의 조화평균
    
  $$ \text {F1 Score} = \frac {2}{\frac {1}{\text{민감도}} + \frac {1}{\text{정밀도}}} = \frac {2}{\frac{\text{TP+FN}} {\text{TP}} + \frac {\text{TP+FP}}{\text{TP}}} = \frac {2 \text{TP}} {2\text {TP} + \text {FP} + \text {FN}} $$

이 6가지 평가 지표들은 반드시 암기하는 수준으로 알고 있어야 합니다. (사실 정확도는 암기하지 않아도 자연스럽게 알고 있는 것이지만, 이 중에서 가장 쓸모없는 것입니다.) 특히 **F1 점수**는 분류모형의 성능 평가 지표가 많이 사용되는 것 중 하나이며, 제가 가장 중요하다고 생각하는데요. 그 이유는 다음과 같습니다.

만약 분류모형이 민감도를 100%로 만들려면 어떻게 하면 될까요? 바로 모두 `Positive`로 예측하면 됩니다. 그렇게 되면 혼동행렬은 `TP` 또는 `FP`만 생길텐데요. 민감도의 분모에 해당하는 `FN`은 0이 되므로 민감도는 100%가 됩니다. 그런데 이렇게 하면 정밀도가 매우 낮은 숫자가 됩니다. 따라서 민감도와 정밀도 모두 좋은 모델이 되러면 `TP`와 `TN`을 늘리는 모형이어야 합니다. 결국 F1 점수가 클수록 분류모형의 성능이 좋습니다.

그런데 왜 산술평균 대신 조화평균을 사용할까요? 만약 산술평균로 계산한다고 가정해보겠습니다. 이 경우, 어느 한 쪽이 높은 갑슬 가지면 (예를 들어 민감도가 100%이면) 비록 다른 한 쪽이 낮은 값을 가지게 되더라도 높은 쪽의 영향을 크게 받으므로 산술평균값은 비교적 큰 값을 갖기 때문입니다. 하지만, 조화평균은 어느 한 쪽이 낮을 경우, 그 결과도 크게 낮아집니다. 조화평균이 평균속도를 내는 방식이라는 점을 상기하면 이해하기 쉽습니다. 예를 들어, 100km 거리를 100km/h로 갔다가 돌아올 때는 25km/h로 왔다면 평균시속은 얼마가 될까요? 직접 계산해보시기 바랍니다.

비록 설명은 장황하였지만, 위의 지표들을 계산하는 함수는 아주 간단합니다. **caret** 패키지의 `confusionMatrix()` 함수를 사용하면 혼동행렬과 각종 지표들을 한 번에 확인할 수 있습니다. 다만 F1 점수는 이 함수로 얻을 수 없으므로 `table()`의 결과값을 사용하여 직접 계산해야 합니다. R 코드는 분류모형 예제에서 다루도록 하겠습니다.

**ROC 커브와 AUROC**
--------------------

ROC는 Receiver Operating Characteristic의 머릿글자인데, 쉽게 말하면 x축은 **1-특이도 (FPR)**, y축은 **민감도 (TPR)**로 놓고 이진분류 모형의 예측 정확도를 평가하는데 사용되는 곡선입니다. **FPR**과 **TPR**은 반비례 관계에 있습니다. 그러니까 분류모형이 `Positive`로 예측한 것 중 `TP`가 증가하면 `FP`는 감소합니다.

![ROC 커브](https://i.stack.imgur.com/PRfzr.png)[3]

위 그림에서 보는 바와 같이 2차원 평면에서 왼쪽 아래 모서리에서 오른쪽 위 모서리까지 점선이 있습니다. 만약 분류모형의 성능이 임의로 뽑는 것과 다르지 않다면 ROC 커브는 이 직선과 겹치게 됩니다. 반대로 분류모형의 성능이 뛰어날수록 ROC 커브는 왼쪽 위 모서리에 가까워집니다. y축이 TRP인 것을 감안하면 실제값이 긍정인 것 중에서 예측모형이 긍정으로 맞춘 비율이 100%에 가까워지기 때문입니다.

AUROC는 Area Under ROC 커브의 머릿글자로 ROC 커브 아래 면적을 의미합니다. 최대값이 1이고, 최소값은 0.5가 됩니다. AUROC 역시 F1 점수처럼 클수록 분류모형의 성능이 좋다는 것을 의미합니다. ROC 커브는 **ROCR** 패키지의 `prediction()` 함수와 `performance()` 함수를 사용하여 그릴 수 있으며, AUROC는 **pROC** 패키지의 `auc()` 함수를 계산할 수 있습니다.

이상으로 분류모형의 성능을 비교하는 주요 지표 2가지를 알아보았습니다. 분류모형을 소개하는 포스팅에서 R 코드를 상세하게 소개하도록 하겠습니다.

[1] 출처 : <https://www.youtube.com/watch?v=AOIkPnKu0YA>

[2] 보다 자세한 내용은 [관련 위키피디아](https://goo.gl/YDJZaY)를 참고하기 바랍니다.

[3] 출처 : <https://goo.gl/1bGcLa>
